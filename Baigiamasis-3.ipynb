{"cells":[{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"fig, ax = plt.subplots(len(data['type'].unique()), sharex=True, figsize=(15,10*len(data['type'].unique())))\n\nk = 0\nfor i in data['type'].unique():\n    data_4 = data[data['type'] == i]\n    wordcloud = WordCloud().generate(data_4['posts'].to_string())\n    ax[k].imshow(wordcloud)\n    ax[k].set_title(i)\n    ax[k].axis(\"off\")\n    k+=1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom nltk.corpus import stopwords \nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom scipy.misc import imread\nfrom wordcloud import WordCloud, STOPWORDS\nfrom numpy import loadtxt\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/mbti_1.csv')\ndata.head()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(40,20))\nplt.xticks(fontsize=24, rotation=0)\nplt.yticks(fontsize=24, rotation=0)\nsns.countplot(data=data, x='type')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"data.groupby('type').agg({'type':'count'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndata['postu_sk'] = data[['posts']].applymap(lambda x: str.count(x, '|||')+1)\ndata.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['post_http'] = data[['posts']].applymap(lambda x: str.count(x, 'http'))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['type']).agg({'post_http':[sum,np.mean]}).sort_values([('post_http', 'sum')])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['post_jpg'] = data[['posts']].applymap(lambda x: str.count(x, 'jpg'))\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#A function is a block of code which only runs when it is called.\n\n#You can pass data, known as parameters, into a function.\n\n#A function can return data as a result\n\n# string.split(s[, sep[, maxsplit]])\n#Return a list of the words of the string s. If the optional second argument sep is absent or None, the words are separated \n#by arbitrary strings of whitespace characters (space, tab, newline, return, formfeed).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef var_row(row):\n    l = []\n    for i in row.split('|||'):\n        l.append(len(i.split()))\n    return np.var(l)\n\ndata['zodziu_sk'] = data['posts'].apply(lambda x: len(x.split()))\n\ndata['zodziu_per_koment'] = data['zodziu_sk'] / data['postu_sk']\n\ndata.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Lemmatizer | Stemmatizer\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer()\nunique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\nlab_encoder = LabelEncoder().fit(unique_type_list) #Encode labels with value between 0 and n_classes-1.\nlist_personality = []  \n\nfor i, row in data.iterrows():\n    \n    # One post\n    OnePost = row.posts\n\n    # Cache the stop words for speed \n    cachedStopWords = stopwords.words(\"english\")\n    \n    #Return the string obtained by replacing the leftmost non-overlapping occurrences of pattern in string by the replacement repl. \n    #If the pattern isn’t found, string is returned unchanged.\n    \n    # List all urls\n    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', OnePost)\n\n    # Remove urls\n    temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', 'link', OnePost)\n\n    # Keep only words\n    temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n\n    # Remove spaces > 1\n    temp = re.sub(' +', ' ', temp).lower()\n     #if remove_stop_words:\n    temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n       # else:\n    #temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')]) #Join all items in a tuple into a string,\n\n    data.loc[i,'pak_tekstas'] = temp\n    \n    data.loc[i, 'pak_tipas'] = lab_encoder.transform([row.type])[0]\n    type_labelized = lab_encoder.transform([row.type])[0]\n    list_personality.append(type_labelized)\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\ndata['pak_tekst_zodziu_sk'] = data['pak_tekstas'].apply(lambda x: len(x.split()))\n\ndata['pak_zodziu_per_koment'] = data['pak_tekst_zodziu_sk'] / data['postu_sk']\n\ndata.head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_types(row):\n    t=row['type']\n\n    I = 0; N = 0\n    T = 0; J = 0\n    \n    if t[0] == 'I': I = 1\n    elif t[0] == 'E': I = 0\n    else: print('I-E incorrect')\n        \n    if t[1] == 'N': N = 1\n    elif t[1] == 'S': N = 0\n    else: print('N-S incorrect')\n        \n    if t[2] == 'T': T = 1\n    elif t[2] == 'F': T = 0\n    else: print('T-F incorrect')\n        \n    if t[3] == 'J': J = 1\n    elif t[3] == 'P': J = 0\n    else: print('J-P incorrect')\n    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n\ndata = data.join(data.apply (lambda row: get_types (row),axis=1))   #apply allow the users to pass a function and apply it on every single \n                                                                    #value of the Pandas series.\ndata.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"Introversion (I) /  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" / \", data['IE'].value_counts()[1])\nprint (\"Intuition (N) – Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" / \", data['NS'].value_counts()[1])\nprint (\"Thinking (T) – Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" / \", data['TF'].value_counts()[1])\nprint (\"Judging (J) – Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" / \", data['JP'].value_counts()[1])\nN = 4\nbottom = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\ntop = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n\nind = np.arange(N)    # grupiu vieta\nwidth = 0.85      # platumas\n\np1 = plt.bar(ind, bottom, width)\np2 = plt.bar(ind, top, width, bottom)\n\nplt.ylabel('Kiekis')\nplt.xlabel('Tipai')\nplt.title('Pasiskirstymas')\n\nplt.xticks(ind, ('I/E',  'N/S', 'T/F', 'J/P',))\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.groupby(['IE']).agg({'pak_tekst_zodziu_sk':[sum,np.mean]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1500, \n                             tokenizer=None,    \n                             preprocessor=None, \n                             stop_words=None,  \n#                             ngram_range=(1,1),\n                             max_df=0.5,\n                             min_df=0.1) \n                                 \ntfizer = TfidfTransformer()\n\nprint(\"CountVectorizer\")\nX_cnt = cntizer.fit_transform(data['pak_tekstas'])\nprint(\"Tf-idf\")\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tfidf\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"X_tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['pak_tipas']\nX = X_tfidf\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = DecisionTreeClassifier(max_depth = 10)\nclf.fit(X_train, y_train)\nclf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score(X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset\nand uses averaging to improve the predictive accuracy and control over-fitting. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(100,1100,100):\n    for j in range(1,11,1):\n        rfc = RandomForestClassifier(n_estimators=i, max_depth=j)\n\n        rfc.fit(X_train,y_train)\n#y_pred=rfc.predict(X_test)\n\n        print(i,j, rfc.score(X_test, y_test))\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Truncated SVD\nsvd = TruncatedSVD(n_components=12, n_iter=7, random_state=42)\nsvd_vec = svd.fit_transform(X_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['IE']\nX = svd_vec\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['IE']\nX = X_tfidf\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['NS']\nX = svd_vec\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['TF']\nX = svd_vec\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['JP']\nX = svd_vec\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = data['JP']\nX = X_tfidf\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nrfc = RandomForestClassifier(n_estimators=1000)\n\nrfc.fit(X_train,y_train)\ny_pred=rfc.predict(X_test)\n\nprint(rfc.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"\n   \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    # transform mbti to binary vector\n    \n    return [b_Pers[l] for l in personality]\n\ndef translate_back(personality):\n    # transform binary vector to mbti personality\n    \n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\n# Check ...\nd = data.head(4)\nlist_personality_bin = np.array([translate_personality(p) for p in d.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n  \nunique_type_list = [x.lower() for x in unique_type_list]\n\n\n# Lemmatize\nstemmer = PorterStemmer()\nlemmatiser = WordNetLemmatizer()\n\n# Cache the stop words for speed \ncachedStopWords = stopwords.words(\"english\")\n\ndef pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n\n    list_personality = []\n    list_posts = []\n    len_data = len(data)\n    i=0\n    \n    for row in data.iterrows():\n        i+=1\n        if (i % 500 == 0 or i == 1 or i == len_data):\n            print(\"%s of %s rows\" % (i, len_data))\n ##### Remove and clean comments\n        posts = row[1].posts\n        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n        temp = re.sub(' +', ' ', temp).lower()\n        if remove_stop_words:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n        else:\n            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n            \n        if remove_mbti_profiles:\n            for t in unique_type_list:\n                temp = temp.replace(t,\"\")\n\n        type_labelized = translate_personality(row[1].type)\n        list_personality.append(type_labelized)\n        list_posts.append(temp)\n\n    list_posts = np.array(list_posts)\n    list_personality = np.array(list_personality)\n    return list_posts, list_personality","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1500, \n                             tokenizer=None,    \n                             preprocessor=None, \n                             stop_words=None,  \n                             max_df=0.7,\n                             min_df=0.1) \n\n# Learn the vocabulary dictionary and return term-document matrix\nprint(\"CountVectorizer...\")\nX_cnt = cntizer.fit_transform(list_posts)\n\n# Transform the count matrix to a normalized tf or tf-idf representation\ntfizer = TfidfTransformer()\n\nprint(\"Tf-idf...\")\n# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = list(enumerate(cntizer.get_feature_names()))\nfeature_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X: Posts in tf-idf representation \\n* 1st row:\\n%s\" % X_tfidf[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type_indicators = [ \"IE: Introversion (I) / Extroversion (E)\", \"NS: Intuition (N) – Sensing (S)\", \n                   \"FT: Feeling (F) - Thinking (T)\", \"JP: Judging (J) – Perceiving (P)\"  ]\n\nfor l in range(len(type_indicators)):\n    print(type_indicators[l])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Posts in tf-idf representation\nX = X_tfidf\n\n# Let's train type indicator individually\nfor l in range(len(type_indicators)):\n    print(\"%s ...\" % (type_indicators[l]))\n    \n    # Let's train type indicator individually\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier()\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"* %s Accuracy: %.2f%%\" % (type_indicators[l], accuracy * 100.0))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}